{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyiqa"
      ],
      "metadata": {
        "id": "UG6ZoxSQbC7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4927d2ac-976a-44e6-d6ff-26c0ab35f61b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyiqa\n",
            "  Downloading pyiqa-0.1.14.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting accelerate<=1.1.0 (from pyiqa)\n",
            "  Downloading accelerate-1.1.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting addict (from pyiqa)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from pyiqa) (4.0.0)\n",
            "Collecting bitsandbytes (from pyiqa)\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from pyiqa) (0.8.1)\n",
            "Collecting facexlib (from pyiqa)\n",
            "  Downloading facexlib-0.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from pyiqa) (1.0.0)\n",
            "Collecting icecream (from pyiqa)\n",
            "  Downloading icecream-2.1.8-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting lmdb (from pyiqa)\n",
            "  Downloading lmdb-1.7.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pyiqa) (2.0.2)\n",
            "Collecting openai-clip (from pyiqa)\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from pyiqa) (4.12.0.88)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from pyiqa) (2.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from pyiqa) (11.3.0)\n",
            "Collecting pre-commit (from pyiqa)\n",
            "  Downloading pre_commit-4.5.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from pyiqa) (8.4.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from pyiqa) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pyiqa) (2.32.4)\n",
            "Requirement already satisfied: ruff in /usr/local/lib/python3.12/dist-packages (from pyiqa) (0.14.8)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from pyiqa) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pyiqa) (1.16.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from pyiqa) (0.2.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from pyiqa) (2.19.0)\n",
            "Requirement already satisfied: timm>=0.8 in /usr/local/lib/python3.12/dist-packages (from pyiqa) (1.0.22)\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.12/dist-packages (from pyiqa) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.13 in /usr/local/lib/python3.12/dist-packages (from pyiqa) (0.24.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from pyiqa) (4.67.1)\n",
            "Collecting transformers==4.37.2 (from pyiqa)\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yapf (from pyiqa)\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.37.2->pyiqa) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.37.2->pyiqa) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.37.2->pyiqa) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.37.2->pyiqa) (2025.11.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.37.2->pyiqa)\n",
            "  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.37.2->pyiqa) (0.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.1.0->pyiqa) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->pyiqa) (3.5.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->pyiqa) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->pyiqa) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->pyiqa) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->pyiqa) (0.70.16)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pyiqa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pyiqa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pyiqa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pyiqa) (2025.11.12)\n",
            "Collecting filterpy (from facexlib->pyiqa)\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from facexlib->pyiqa) (0.60.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from facexlib->pyiqa) (4.12.0.88)\n",
            "Collecting colorama>=0.3.9 (from icecream->pyiqa)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from icecream->pyiqa) (2.19.2)\n",
            "Collecting executing>=2.1.0 (from icecream->pyiqa)\n",
            "  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.0.1 (from icecream->pyiqa)\n",
            "  Downloading asttokens-3.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting ftfy (from openai-clip->pyiqa)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->pyiqa) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->pyiqa) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->pyiqa) (2025.2)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->pyiqa)\n",
            "  Downloading cfgv-3.5.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->pyiqa)\n",
            "  Downloading identify-2.6.15-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->pyiqa)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->pyiqa)\n",
            "  Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->pyiqa) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->pyiqa) (1.6.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->pyiqa) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->pyiqa) (2025.10.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->pyiqa) (0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->pyiqa) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->pyiqa) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->pyiqa) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->pyiqa) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->pyiqa) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->pyiqa) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->pyiqa) (3.1.4)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf->pyiqa) (4.5.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->pyiqa) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2->pyiqa) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.12->pyiqa) (1.3.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->pyiqa)\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->pyiqa) (3.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from filterpy->facexlib->pyiqa) (3.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->openai-clip->pyiqa) (0.2.14)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->facexlib->pyiqa) (0.43.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->pyiqa) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->pyiqa) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->pyiqa) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->pyiqa) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->pyiqa) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->pyiqa) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->pyiqa) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy->facexlib->pyiqa) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy->facexlib->pyiqa) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy->facexlib->pyiqa) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy->facexlib->pyiqa) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy->facexlib->pyiqa) (3.2.5)\n",
            "Downloading pyiqa-0.1.14.1-py3-none-any.whl (276 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m276.2/276.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.1.0-py3-none-any.whl (333 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading facexlib-0.3.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading icecream-2.1.8-py3-none-any.whl (15 kB)\n",
            "Downloading lmdb-1.7.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (299 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m299.4/299.4 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.5.0-py2.py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asttokens-3.0.1-py3-none-any.whl (27 kB)\n",
            "Downloading cfgv-3.5.0-py2.py3-none-any.whl (7.4 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\n",
            "Downloading identify-2.6.15-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m125.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m138.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-clip, filterpy\n",
            "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=ed8e34506657a5ca5d46bf491cf9b85b84e1761c6ebfcc3441fd9fc32b88e4f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/49/bc/c2342e8e14878210ba4825cf314a53f2570f6fb18b91fce3cf\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110460 sha256=906579c8b1fbcbaaf15c592c653f28d873b0083acb4a4340e6806cbe3b683c90\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/bf/4c/b0c3f4798a0166668752312a67118b27a3cd341e13ac0ae6ee\n",
            "Successfully built openai-clip filterpy\n",
            "Installing collected packages: lmdb, distlib, addict, yapf, virtualenv, nodeenv, identify, ftfy, executing, colorama, cfgv, asttokens, pre-commit, openai-clip, icecream, tokenizers, filterpy, transformers, bitsandbytes, accelerate, facexlib, pyiqa\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.1.0 addict-2.4.0 asttokens-3.0.1 bitsandbytes-0.48.2 cfgv-3.5.0 colorama-0.4.6 distlib-0.4.0 executing-2.2.1 facexlib-0.3.0 filterpy-1.4.5 ftfy-6.3.1 icecream-2.1.8 identify-2.6.15 lmdb-1.7.5 nodeenv-1.9.1 openai-clip-1.0.1 pre-commit-4.5.0 pyiqa-0.1.14.1 tokenizers-0.15.2 transformers-4.37.2 virtualenv-20.35.4 yapf-0.43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fnv-I71I4gGT",
        "outputId": "de86ab75-0f3c-41d9-d82b-e1287b51b773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Working directory ready: /content/drive/My Drive/ECE253_Project\n",
            "ğŸ“‚ Created dedicated Fine-Tune output folder: /content/drive/My Drive/ECE253_Project/5_Output_NAFNet_FineTuned\n",
            ">>> Installing dependencies...\n",
            "âœ… Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Environment Setup & Drive Mounting (Updated)\n",
        "# ====================================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the project root directory\n",
        "PROJECT_ROOT = \"/content/drive/My Drive/ECE253_Project\"\n",
        "\n",
        "# 3. Define sub-directories\n",
        "DIRS = {\n",
        "    \"GT\": os.path.join(PROJECT_ROOT, \"1_GroundTruth\"),      # Folder for Sharp images\n",
        "    \"BLUR\": os.path.join(PROJECT_ROOT, \"2_InputBlur\"),      # Folder for Blurred images\n",
        "    \"RL_OUT\": os.path.join(PROJECT_ROOT, \"3_Output_RL\"),    # Folder for RL results\n",
        "    \"NAF_OUT\": os.path.join(PROJECT_ROOT, \"4_Output_NAFNet\"), # Folder for Pre-trained NAFNet results\n",
        "    \"NAF_FT_OUT\": os.path.join(PROJECT_ROOT, \"5_Output_NAFNet_FineTuned\"),\n",
        "\n",
        "    \"METRICS\": os.path.join(PROJECT_ROOT, \"Metrics\")        # Folder for CSV files\n",
        "}\n",
        "\n",
        "# 4. Create directories if they don't exist\n",
        "for k, v in DIRS.items():\n",
        "    os.makedirs(v, exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Working directory ready: {PROJECT_ROOT}\")\n",
        "print(f\"ğŸ“‚ Created dedicated Fine-Tune output folder: {DIRS['NAF_FT_OUT']}\")\n",
        "\n",
        "# 5. Install necessary libraries\n",
        "print(\">>> Installing dependencies...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\", \"scikit-image\", \"tqdm\", \"pandas\", \"opencv-python\"])\n",
        "print(\"âœ… Dependencies installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: NAFNet Architecture & Pre-trained Weight Loading\n",
        "# ========================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# --- 1. NAFNet Architecture Definitions ---\n",
        "\n",
        "class LayerNormFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, weight, bias, eps):\n",
        "        ctx.eps = eps\n",
        "        N, C, H, W = x.size()\n",
        "        mu = x.mean(1, keepdim=True)\n",
        "        var = (x - mu).pow(2).mean(1, keepdim=True)\n",
        "        y = (x - mu) / (var + eps).sqrt()\n",
        "        ctx.save_for_backward(y, var, weight)\n",
        "        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        eps = ctx.eps\n",
        "        N, C, H, W = grad_output.size()\n",
        "        y, var, weight = ctx.saved_tensors\n",
        "        g = grad_output * weight.view(1, C, 1, 1)\n",
        "        mean_g = g.mean(dim=1, keepdim=True)\n",
        "        mean_gy = (g * y).mean(dim=1, keepdim=True)\n",
        "        gx = 1. / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)\n",
        "        return gx, (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0), grad_output.sum(dim=3).sum(dim=2).sum(dim=0), None\n",
        "\n",
        "class LayerNorm2d(nn.Module):\n",
        "    def __init__(self, channels, eps=1e-6):\n",
        "        super(LayerNorm2d, self).__init__()\n",
        "        self.register_parameter('weight', nn.Parameter(torch.ones(channels)))\n",
        "        self.register_parameter('bias', nn.Parameter(torch.zeros(channels)))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)\n",
        "\n",
        "class SimpleGate(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x1, x2 = x.chunk(2, dim=1)\n",
        "        return x1 * x2\n",
        "\n",
        "class NAFBlock(nn.Module):\n",
        "    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0.):\n",
        "        super().__init__()\n",
        "        dw_channel = c * DW_Expand\n",
        "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "        self.conv2 = nn.Conv2d(in_channels=dw_channel, out_channels=dw_channel, kernel_size=3, padding=1, stride=1, groups=dw_channel, bias=True)\n",
        "        self.conv3 = nn.Conv2d(in_channels=dw_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "        self.sca = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size=1, padding=0, stride=1, groups=1, bias=True),\n",
        "        )\n",
        "        self.conv4 = nn.Conv2d(in_channels=c, out_channels=c * FFN_Expand, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "        self.conv5 = nn.Conv2d(in_channels=c * FFN_Expand // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "        self.norm1 = LayerNorm2d(c)\n",
        "        self.norm2 = LayerNorm2d(c)\n",
        "        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
        "        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
        "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
        "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        inp = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = SimpleGate()(x)\n",
        "        x = x * self.sca(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.dropout1(x)\n",
        "        y = inp + x * self.beta\n",
        "        x = self.norm2(y)\n",
        "        x = self.conv4(x)\n",
        "        x = SimpleGate()(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.dropout2(x)\n",
        "        return y + x * self.gamma\n",
        "\n",
        "class NAFNet(nn.Module):\n",
        "    def __init__(self, img_channel=3, width=16, middle_blk_num=1, enc_blk_nums=[], dec_blk_nums=[]):\n",
        "        super().__init__()\n",
        "        self.intro = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1, bias=True)\n",
        "        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1, bias=True)\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.decoders = nn.ModuleList()\n",
        "        self.middle_blks = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        chan = width\n",
        "        for num in enc_blk_nums:\n",
        "            self.encoders.append(nn.Sequential(*[NAFBlock(chan) for _ in range(num)]))\n",
        "            self.downs.append(nn.Conv2d(chan, 2*chan, 2, 2))\n",
        "            chan = chan * 2\n",
        "        self.middle_blks = nn.Sequential(*[NAFBlock(chan) for _ in range(middle_blk_num)])\n",
        "        for num in dec_blk_nums:\n",
        "            self.ups.append(nn.Sequential(nn.Conv2d(chan, chan * 2, 1, bias=False), nn.PixelShuffle(2)))\n",
        "            chan = chan // 2\n",
        "            self.decoders.append(nn.Sequential(*[NAFBlock(chan) for _ in range(num)]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        inp = x\n",
        "        x = self.intro(x)\n",
        "        encs = []\n",
        "        for encoder, down in zip(self.encoders, self.downs):\n",
        "            x = encoder(x)\n",
        "            encs.append(x)\n",
        "            x = down(x)\n",
        "        x = self.middle_blks(x)\n",
        "        for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):\n",
        "            x = up(x)\n",
        "            x = x + enc_skip\n",
        "            x = decoder(x)\n",
        "        x = self.ending(x)\n",
        "        return x + inp\n",
        "\n",
        "print(\"âœ… NAFNet architecture defined.\")\n",
        "\n",
        "# --- 2. Load Pre-trained Weights ---\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\">>> Running on device: {device}\")\n",
        "\n",
        "# Initialize Model (Width=32 configuration for GoPro)\n",
        "model_naf = NAFNet(img_channel=3, width=32, middle_blk_num=1,\n",
        "                   enc_blk_nums=[1, 1, 1, 28], dec_blk_nums=[1, 1, 1, 1])\n",
        "\n",
        "# Download and Load Weights\n",
        "weight_path = \"NAFNet-GoPro-width32.pth\"\n",
        "weight_url = \"https://huggingface.co/nyanko7/nafnet-models/resolve/main/NAFNet-GoPro-width32.pth\"\n",
        "\n",
        "if not os.path.exists(weight_path):\n",
        "    print(\">>> Downloading pre-trained weights from HuggingFace...\")\n",
        "    os.system(f\"curl -L -o {weight_path} {weight_url}\")\n",
        "\n",
        "print(\">>> Loading weights into model...\")\n",
        "try:\n",
        "    checkpoint = torch.load(weight_path, map_location=device)\n",
        "    param_dict = checkpoint['params'] if 'params' in checkpoint else checkpoint\n",
        "\n",
        "    # strict=False allows us to load weights even if there are minor mismatches\n",
        "    # (though here they should match perfectly)\n",
        "    model_naf.load_state_dict(param_dict, strict=False)\n",
        "    model_naf.to(device)\n",
        "    print(\"âœ… Model loaded successfully and moved to GPU.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading weights: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txKYx2VJ4rpz",
        "outputId": "69d53b93-46e3-47ff-b099-4ae8f321d41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… NAFNet architecture defined.\n",
            ">>> Running on device: cuda\n",
            ">>> Downloading pre-trained weights from HuggingFace...\n",
            ">>> Loading weights into model...\n",
            "âœ… Model loaded successfully and moved to GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.3: Fine-tuning (Train Split: First 80 Images)\n",
        "# ====================================================\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "# 1. Define Dataset with explicit splitting\n",
        "class TrafficTrainDataset(Dataset):\n",
        "    def __init__(self, gt_dir, blur_dir, patch_size=256):\n",
        "        self.gt_dir = gt_dir\n",
        "        self.blur_dir = blur_dir\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Sort files to ensure deterministic order\n",
        "        all_files = sorted(os.listdir(gt_dir))\n",
        "\n",
        "        # Select ONLY the first 80 images for training\n",
        "        self.files = all_files[:80]\n",
        "        print(f\">>> Dataset [TRAIN]: Loaded {len(self.files)} images (traffic_0000 - traffic_0079).\")\n",
        "\n",
        "    def __len__(self): return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.files[idx]\n",
        "\n",
        "        # Load images\n",
        "        img_gt = cv2.cvtColor(cv2.imread(os.path.join(self.gt_dir, name)), cv2.COLOR_BGR2RGB)\n",
        "        img_blur = cv2.cvtColor(cv2.imread(os.path.join(self.blur_dir, name)), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Normalize\n",
        "        img_gt = img_gt.astype(np.float32) / 255.0\n",
        "        img_blur = img_blur.astype(np.float32) / 255.0\n",
        "\n",
        "        # Random Crop (Data Augmentation for training)\n",
        "        h, w, c = img_gt.shape\n",
        "        if h > self.patch_size and w > self.patch_size:\n",
        "            rnd_h = random.randint(0, h - self.patch_size)\n",
        "            rnd_w = random.randint(0, w - self.patch_size)\n",
        "            img_gt = img_gt[rnd_h:rnd_h+self.patch_size, rnd_w:rnd_w+self.patch_size, :]\n",
        "            img_blur = img_blur[rnd_h:rnd_h+self.patch_size, rnd_w:rnd_w+self.patch_size, :]\n",
        "        else:\n",
        "            img_gt = cv2.resize(img_gt, (self.patch_size, self.patch_size))\n",
        "            img_blur = cv2.resize(img_blur, (self.patch_size, self.patch_size))\n",
        "\n",
        "        return (torch.from_numpy(img_blur.transpose(2,0,1)),\n",
        "                torch.from_numpy(img_gt.transpose(2,0,1)))\n",
        "\n",
        "# 2. Setup DataLoader\n",
        "# Use DIRS from your existing environment setup\n",
        "train_ds = TrafficTrainDataset(DIRS[\"GT\"], DIRS[\"BLUR\"])\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# Optimizer & Loss\n",
        "optimizer = optim.AdamW(model_naf.parameters(), lr=1e-4, betas=(0.9, 0.9)) # Lower learning rate for fine-tuning\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# 3. Training Loop\n",
        "EPOCHS = 5\n",
        "print(f\"\\n>>> Starting Fine-tuning for {EPOCHS} epochs...\")\n",
        "\n",
        "model_naf.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    for blur, gt in loop:\n",
        "        blur = blur.to(device)\n",
        "        gt = gt.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = model_naf(blur)\n",
        "        loss = criterion(output, gt)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model_naf.parameters(), 0.01)\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(loss=f\"{loss.item():.5f}\")\n",
        "\n",
        "print(\"âœ… Cell 3.3: Fine-tuning complete. Model weights updated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQAyYt-l9A6D",
        "outputId": "b60feac6-283c-47c7-c127-685b58825753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Dataset [TRAIN]: Loaded 80 images (traffic_0000 - traffic_0079).\n",
            "\n",
            ">>> Starting Fine-tuning for 5 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.18it/s, loss=0.00895]\n",
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:07<00:00,  2.51it/s, loss=0.01475]\n",
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:07<00:00,  2.62it/s, loss=0.00963]\n",
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:07<00:00,  2.50it/s, loss=0.00989]\n",
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:07<00:00,  2.56it/s, loss=0.01002]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 3.3: Fine-tuning complete. Model weights updated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.4: Evaluation with Smart Metric Selection (PSNR/SSIM if GT exists, NIQE otherwise)\n",
        "# =========================================================================================\n",
        "import pandas as pd\n",
        "from skimage import metrics\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch\n",
        "import pyiqa  # <--- NEW: Import PyIQA\n",
        "\n",
        "# 1. Initialize NIQE Metric\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\">>> Loading NIQE metric...\")\n",
        "try:\n",
        "    niqe_metric = pyiqa.create_metric('niqe', device=device)\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Warning: Could not load NIQE: {e}\")\n",
        "    niqe_metric = None\n",
        "\n",
        "model_naf.eval()\n",
        "results_naf = []\n",
        "\n",
        "# --- Split Logic: Updated to include ALL remaining images ---\n",
        "# 0-79: Used for Training\n",
        "# 80-99: Synthetic Validation (Has GT)\n",
        "# 100+: Real World Images (No GT)\n",
        "all_files = sorted(os.listdir(DIRS[\"BLUR\"]))\n",
        "test_files = all_files[80:] # <--- NEW: Select EVERYTHING after the training set\n",
        "\n",
        "print(f\">>> Evaluation Set: {len(test_files)} images (traffic_0080 onwards).\")\n",
        "print(f\">>> Saving results to: {DIRS['NAF_FT_OUT']}\")\n",
        "\n",
        "for f in tqdm(test_files, desc=\"NAFNet Inference\"):\n",
        "    # Paths\n",
        "    path_blur = os.path.join(DIRS[\"BLUR\"], f)\n",
        "    path_gt = os.path.join(DIRS[\"GT\"], f)\n",
        "    path_out = os.path.join(DIRS[\"NAF_FT_OUT\"], f)\n",
        "\n",
        "    # Check if GT exists\n",
        "    has_gt = os.path.exists(path_gt)\n",
        "\n",
        "    # Read Blur Image\n",
        "    img_blur_orig = cv2.imread(path_blur)\n",
        "    if img_blur_orig is None: continue # Skip if file is corrupt\n",
        "    img_blur_orig = cv2.cvtColor(img_blur_orig, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Preprocess\n",
        "    img_tensor = torch.from_numpy(img_blur_orig.astype(np.float32)/255.0)\n",
        "    img_tensor = img_tensor.permute(2,0,1).unsqueeze(0).to(device)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        # Handle padding (Network requires multiple of 32)\n",
        "        _, _, h, w = img_tensor.shape\n",
        "        h_n = (h // 32) * 32\n",
        "        w_n = (w // 32) * 32\n",
        "        inp = img_tensor[:, :, :h_n, :w_n]\n",
        "\n",
        "        output = model_naf(inp)\n",
        "\n",
        "    # Post-process\n",
        "    out_img = output.squeeze().cpu().permute(1,2,0).numpy()\n",
        "    out_img = np.clip(out_img, 0, 1)\n",
        "    out_uint8 = (out_img * 255.0).astype(np.uint8)\n",
        "\n",
        "    # Save Image to the new folder\n",
        "    cv2.imwrite(path_out, cv2.cvtColor(out_uint8, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # --- Metrics Calculation Logic ---\n",
        "    row = {\"Image\": f, \"Method\": \"NAFNet_FineTuned\", \"Has_GT\": has_gt}\n",
        "\n",
        "    # 1. Calculate NIQE (Applicable for ALL images)\n",
        "    if niqe_metric:\n",
        "        # NIQE expects tensor input (NCHW)\n",
        "        out_tensor = torch.from_numpy(out_uint8).permute(2,0,1).unsqueeze(0).float() / 255.0\n",
        "        row[\"NIQE\"] = niqe_metric(out_tensor.to(device)).item()\n",
        "    else:\n",
        "        row[\"NIQE\"] = None\n",
        "\n",
        "    # 2. Calculate PSNR/SSIM (Only if GT exists)\n",
        "    if has_gt:\n",
        "        img_gt = cv2.imread(path_gt)\n",
        "        if img_gt is not None:\n",
        "            img_gt = cv2.cvtColor(img_gt, cv2.COLOR_BGR2RGB)\n",
        "            # Crop GT to match output size (due to padding)\n",
        "            gt_crop = img_gt[:h_n, :w_n, :]\n",
        "\n",
        "            row[\"PSNR\"] = metrics.peak_signal_noise_ratio(gt_crop, out_uint8)\n",
        "            row[\"SSIM\"] = metrics.structural_similarity(gt_crop, out_uint8, channel_axis=2, win_size=3)\n",
        "        else:\n",
        "            row[\"PSNR\"] = None\n",
        "            row[\"SSIM\"] = None\n",
        "    else:\n",
        "        # No GT available (Real-world images)\n",
        "        row[\"PSNR\"] = None\n",
        "        row[\"SSIM\"] = None\n",
        "\n",
        "    results_naf.append(row)\n",
        "\n",
        "# Save Metrics with a DISTINCT filename\n",
        "csv_path = os.path.join(DIRS[\"METRICS\"], \"metrics_nafnet_finetuned_mixed.csv\")\n",
        "pd.DataFrame(results_naf).to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Cell 3.4 Complete!\")\n",
        "print(f\"   - Images saved to: {DIRS['NAF_FT_OUT']}\")\n",
        "print(f\"   - Metrics saved to: {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-xDLtOS9COI",
        "outputId": "66b3b7d4-f532-489b-9508-ec871f436ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Loading NIQE metric...\n",
            "Downloading: \"https://huggingface.co/chaofengc/IQA-PyTorch-Weights/resolve/main/niqe_modelparameters.mat\" to /root/.cache/torch/hub/pyiqa/niqe_modelparameters.mat\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.15k/8.15k [00:00<00:00, 34.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Evaluation Set: 30 images (traffic_0080 onwards).\n",
            ">>> Saving results to: /content/drive/My Drive/ECE253_Project/5_Output_NAFNet_FineTuned\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "NAFNet Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:26<00:00,  1.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Cell 3.4 Complete!\n",
            "   - Images saved to: /content/drive/My Drive/ECE253_Project/5_Output_NAFNet_FineTuned\n",
            "   - Metrics saved to: /content/drive/My Drive/ECE253_Project/Metrics/metrics_nafnet_finetuned_mixed.csv\n"
          ]
        }
      ]
    }
  ]
}